{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перевод данных из датасета в массивы\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "h, w = 48, 48\n",
    "\n",
    "dataset_dir = 'datasets/fer2013.csv'\n",
    "dataset_plus_dir = 'datasets/fer2013new.csv'\n",
    "dataset = pd.read_csv(dataset_dir)\n",
    "dataset_plus = pd.read_csv(dataset_plus_dir)\n",
    "\n",
    "drop_indices = dataset_plus.index[\n",
    "#       (dataset_plus['neutral'] < 10) &\n",
    "      (dataset_plus['happiness'] < 10)\n",
    "    & (dataset_plus['surprise'] < 9)\n",
    "    & (dataset_plus['sadness'] < 7)\n",
    "    & (dataset_plus['anger'] < 7)\n",
    "#     & (dataset_plus['disgust'] < 10)\n",
    "#     & (dataset_plus['fear'] < 10)\n",
    "    | (dataset_plus['contempt'] > 1)\n",
    "    | (dataset_plus['unknown'] > 1)\n",
    "    | (dataset_plus['NF'] > 0)]\n",
    "dataset = dataset.drop(drop_indices, axis=0)\n",
    "drop_indices = dataset.index[(dataset['emotion'] == 1) | (dataset['emotion'] == 2) | (dataset['emotion'] == 6)]\n",
    "dataset = dataset.drop(drop_indices, axis=0)\n",
    "drop_indices = np.random.choice(dataset.index[dataset['emotion'] == 3], 3800, replace=False)\n",
    "dataset = dataset.drop(drop_indices, axis=0)\n",
    "\n",
    "# num_train_samples = len(dataset[dataset['Usage'] == 'Training'])\n",
    "# num_val_samples = len(dataset[dataset['Usage'] == 'PublicTest'])\n",
    "# num_test_samples = len(dataset[dataset['Usage'] == 'PrivateTest'])\n",
    "\n",
    "num_train_samples = len(dataset) - 2000\n",
    "num_val_samples = 1000\n",
    "num_test_samples = 1000\n",
    "\n",
    "train_images = np.zeros((num_train_samples, h, w))\n",
    "validation_images = np.zeros((num_val_samples, h, w))\n",
    "test_images = np.zeros((num_test_samples, h, w))\n",
    "\n",
    "train_labels = np.zeros(num_train_samples)\n",
    "validation_labels = np.zeros(num_val_samples)\n",
    "test_labels = np.zeros(num_test_samples)\n",
    "\n",
    "# i, j, k = 0, 0, 0\n",
    "# for index, row in dataset.iterrows():\n",
    "    \n",
    "#     if row['Usage'] == 'Training':\n",
    "#         train_images[i] = np.fromstring(row['pixels'], dtype=np.uint8, sep=' ').reshape(h, w)\n",
    "#         train_labels[i] = row['emotion']\n",
    "#         i += 1\n",
    "\n",
    "#     elif row['Usage'] == 'PublicTest':\n",
    "#         validation_images[j] = np.fromstring(row['pixels'], dtype=np.uint8, sep=' ').reshape(h, w)\n",
    "#         validation_labels[j] = row['emotion']\n",
    "#         j += 1\n",
    "\n",
    "#     elif row['Usage'] == 'PrivateTest':\n",
    "#         test_images[k] = np.fromstring(row['pixels'], dtype=np.uint8, sep=' ').reshape(h, w)\n",
    "#         test_labels[k] = row['emotion']\n",
    "#         k += 1\n",
    "\n",
    "i, j, k, f = 0, 0, 0, 0\n",
    "for index, row in dataset.iterrows():\n",
    "    \n",
    "    if (i < 3000) | (i >= 5000):\n",
    "        train_images[j] = np.fromstring(row['pixels'], dtype=np.uint8, sep=' ').reshape(h, w)\n",
    "        train_labels[j] = row['emotion']\n",
    "        j += 1\n",
    "\n",
    "    elif (i >= 3000) & (i < 4000):\n",
    "        validation_images[k] = np.fromstring(row['pixels'], dtype=np.uint8, sep=' ').reshape(h, w)\n",
    "        validation_labels[k] = row['emotion']\n",
    "        k += 1\n",
    "\n",
    "    elif (i >= 4000) & (i < 5000):\n",
    "        test_images[f] = np.fromstring(row['pixels'], dtype=np.uint8, sep=' ').reshape(h, w)\n",
    "        test_labels[f] = row['emotion']\n",
    "        f += 1\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных к передаче в нейросеть\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_images = np.kron(train_images, np.ones((2, 2)))\n",
    "validation_images = np.kron(validation_images, np.ones((2, 2)))\n",
    "test_images = np.kron(test_images, np.ones((2, 2)))\n",
    "\n",
    "train_images = np.stack((train_images,) * 3, 3)\n",
    "validation_images = np.stack((validation_images,) * 3, 3)\n",
    "test_images = np.stack((test_images,) * 3, 3)\n",
    "\n",
    "train_images = train_images.astype('float32') / 255\n",
    "validation_images = validation_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "i = 0\n",
    "for label in train_labels:\n",
    "    if label > 2:\n",
    "        train_labels[i] -= 2\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "for label in validation_labels:\n",
    "    if label > 2:\n",
    "        validation_labels[i] -= 2\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "for label in test_labels:\n",
    "    if label > 2:\n",
    "        test_labels[i] -= 2\n",
    "    i += 1\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "validation_labels = to_categorical(validation_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание генераторов\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "validation_gen = ImageDataGenerator()\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "train_gen = train_gen.flow(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    batch_size = batch_size)\n",
    "\n",
    "validation_gen = validation_gen.flow(\n",
    "    validation_images,\n",
    "    validation_labels,\n",
    "    batch_size = batch_size)\n",
    "\n",
    "test_gen = test_gen.flow(\n",
    "    test_images,\n",
    "    test_labels,\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Замороженная VGG16\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications import VGG16\n",
    "import math\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(vgg16)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_vgg16_fr.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model = load_model('ferCNN_vgg16_fr.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Размороженная VGG16\n",
    "\n",
    "number_of_epochs = 70\n",
    "\n",
    "set_trainable = False\n",
    "for layer in vgg16.layers:\n",
    "    if layer.name == 'block4_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_vgg16_unfr.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model = load_model('ferCNN_vgg16_unfr.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Замороженная VGG19\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications import VGG19\n",
    "import math\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "vgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(vgg19)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_vgg19_fr.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model = load_model('ferCNN_vgg19_fr.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Размороженная VGG19\n",
    "\n",
    "number_of_epochs = 70\n",
    "\n",
    "set_trainable = False\n",
    "for layer in vgg19.layers:\n",
    "    if layer.name == 'block4_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_vgg19_unfr.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model = load_model('ferCNN_vgg19_unfr.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замороженная ResNet50\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications import ResNet50\n",
    "import math\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "resnet50.trainable = False\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(resnet50)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_resnet50_fr.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model = load_model('ferCNN_resnet50_fr.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Размороженная ResNet50\n",
    "\n",
    "number_of_epochs = 70\n",
    "\n",
    "resnet50.trainable = True\n",
    "set_trainable = False\n",
    "for layer in resnet50.layers:\n",
    "    if layer.name == 'res3a_branch2a':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_resnet50_unfr.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model = load_model('ferCNN_resnet50_unfr.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замороженная InceptionV3\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications import InceptionV3\n",
    "import math\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "inceptionv3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "inceptionv3.trainable = False\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(inceptionv3)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_inceptionv3_fr.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model = load_model('ferCNN_inceptionv3_fr.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размороженная InceptionV3\n",
    "\n",
    "number_of_epochs = 70\n",
    "\n",
    "inceptionv3.trainable = True\n",
    "# set_trainable = False\n",
    "# for layer in inceptionv3.layers:\n",
    "#     if layer.name == 'mixed6':\n",
    "#         set_trainable = True\n",
    "#     if set_trainable:\n",
    "#         layer.trainable = True\n",
    "#     else:\n",
    "#         layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_inceptionv3_unfr.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model = load_model('ferCNN_inceptionv3_unfr.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводит графики точности и потерь\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "plt.plot(epochs, train_acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('acc.png', bbox_inches='tight')\n",
    "plt.savefig('acc.pdf', bbox_inches='tight')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('loss.png', bbox_inches='tight')\n",
    "plt.savefig('loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводит сглаженные графики точности и потерь\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def smooth_curve(points, factor=0.8):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "plt.plot(epochs, smooth_curve(train_acc), 'bo', label='Smoothed training acc')\n",
    "plt.plot(epochs, smooth_curve(val_acc), 'r', label='Smoothed validation acc')\n",
    "plt.title('Smoothed training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('smoothed_acc.png', bbox_inches='tight')\n",
    "plt.savefig('smoothed_acc.pdf', bbox_inches='tight')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, smooth_curve(train_loss), 'bo', label='Smoothed training loss')\n",
    "plt.plot(epochs, smooth_curve(val_loss), 'r', label='Smoothed validation loss')\n",
    "plt.title('Smoothed training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('smoothed_loss.png', bbox_inches='tight')\n",
    "plt.savefig('smoothed_loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование нейросети\n",
    "\n",
    "from keras.models import load_model\n",
    "import math\n",
    "\n",
    "model = load_model('ferCNN_vgg16_fr.h5')\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Готовое приложение emotion_recognizer_v1.1.py\n",
    "\n",
    "# %%writefile emotion_recognizer.py\n",
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "import ntpath\n",
    "import os\n",
    "\n",
    "class Ui_MainWindow(object):\n",
    "    model_path, image_path = \"\", \"\"\n",
    "\n",
    "    def setupUi(self, MainWindow):\n",
    "        MainWindow.setObjectName(\"MainWindow\")\n",
    "        MainWindow.resize(600, 500)\n",
    "        self.centralwidget = QtWidgets.QWidget(MainWindow)\n",
    "        self.centralwidget.setObjectName(\"centralwidget\")\n",
    "        self.gridLayout = QtWidgets.QGridLayout(self.centralwidget)\n",
    "        self.gridLayout.setObjectName(\"gridLayout\")\n",
    "        self.selectedImageLbl = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.selectedImageLbl.setMinimumSize(QtCore.QSize(150, 150))\n",
    "        self.selectedImageLbl.setFrameShape(QtWidgets.QFrame.StyledPanel)\n",
    "        self.selectedImageLbl.setText(\"\")\n",
    "        self.selectedImageLbl.setObjectName(\"selectedImageLbl\")\n",
    "        self.gridLayout.addWidget(self.selectedImageLbl, 0, 0, 1, 4)\n",
    "        self.modelTextLbl = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.modelTextLbl.setMaximumSize(QtCore.QSize(16777215, 30))\n",
    "        font = QtGui.QFont()\n",
    "        font.setPointSize(11)\n",
    "        self.modelTextLbl.setFont(font)\n",
    "        self.modelTextLbl.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.modelTextLbl.setObjectName(\"modelTextLbl\")\n",
    "        self.gridLayout.addWidget(self.modelTextLbl, 2, 0, 1, 1)\n",
    "        self.emotionTextLbl = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.emotionTextLbl.setMaximumSize(QtCore.QSize(16777215, 30))\n",
    "        font = QtGui.QFont()\n",
    "        font.setPointSize(11)\n",
    "        self.emotionTextLbl.setFont(font)\n",
    "        self.emotionTextLbl.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.emotionTextLbl.setObjectName(\"emotionTextLbl\")\n",
    "        self.gridLayout.addWidget(self.emotionTextLbl, 1, 0, 1, 1)\n",
    "        self.modelNameLbl = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.modelNameLbl.setMaximumSize(QtCore.QSize(16777215, 30))\n",
    "        font = QtGui.QFont()\n",
    "        font.setPointSize(11)\n",
    "        self.modelNameLbl.setFont(font)\n",
    "        self.modelNameLbl.setFrameShape(QtWidgets.QFrame.StyledPanel)\n",
    "        self.modelNameLbl.setText(\"\")\n",
    "        self.modelNameLbl.setObjectName(\"modelNameLbl\")\n",
    "        self.gridLayout.addWidget(self.modelNameLbl, 2, 1, 1, 3)\n",
    "        self.emotionNameLbl = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.emotionNameLbl.setMaximumSize(QtCore.QSize(16777215, 30))\n",
    "        font = QtGui.QFont()\n",
    "        font.setPointSize(11)\n",
    "        self.emotionNameLbl.setFont(font)\n",
    "        self.emotionNameLbl.setFrameShape(QtWidgets.QFrame.StyledPanel)\n",
    "        self.emotionNameLbl.setText(\"\")\n",
    "        self.emotionNameLbl.setObjectName(\"emotionNameLbl\")\n",
    "        self.gridLayout.addWidget(self.emotionNameLbl, 1, 1, 1, 3)\n",
    "        self.selectImageBtn = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.selectImageBtn.setMinimumSize(QtCore.QSize(0, 35))\n",
    "        font = QtGui.QFont()\n",
    "        font.setPointSize(11)\n",
    "        self.selectImageBtn.setFont(font)\n",
    "        self.selectImageBtn.setObjectName(\"selectImageBtn\")\n",
    "        self.gridLayout.addWidget(self.selectImageBtn, 3, 0, 1, 2)\n",
    "        self.recognizeEmotionBtn = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.recognizeEmotionBtn.setMinimumSize(QtCore.QSize(0, 35))\n",
    "        font = QtGui.QFont()\n",
    "        font.setPointSize(11)\n",
    "        self.recognizeEmotionBtn.setFont(font)\n",
    "        self.recognizeEmotionBtn.setObjectName(\"recognizeEmotionBtn\")\n",
    "        self.gridLayout.addWidget(self.recognizeEmotionBtn, 3, 2, 1, 2)\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "\n",
    "        self.retranslateUi(MainWindow)\n",
    "        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n",
    "\n",
    "        self.selectImageBtn.clicked.connect(self.setImage)\n",
    "        self.recognizeEmotionBtn.clicked.connect(self.recognizeEmotion)\n",
    "\n",
    "        for file in os.listdir(sys.path[0]):\n",
    "            file_path = os.path.join(sys.path[0], file)\n",
    "            if os.path.isfile(file_path) & (file.endswith('.h5')):\n",
    "                self.model_path = file_path\n",
    "                self.modelNameLbl.setText(file)\n",
    "\n",
    "    def retranslateUi(self, MainWindow):\n",
    "        _translate = QtCore.QCoreApplication.translate\n",
    "        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"Распознаватель эмоций\"))\n",
    "        self.modelTextLbl.setText(_translate(\"MainWindow\", \"Модель:\"))\n",
    "        self.emotionTextLbl.setText(_translate(\"MainWindow\", \"Эмоция:\"))\n",
    "        self.selectImageBtn.setText(_translate(\"MainWindow\", \"Выбрать изображение\"))\n",
    "        self.recognizeEmotionBtn.setText(_translate(\"MainWindow\", \"Распознать эмоцию\"))\n",
    "\n",
    "    def setImage(self):\n",
    "        imagePath, _ = QtWidgets.QFileDialog.getOpenFileName(None, \"Select Image\", \"\", \"Image Files (*.png *.jpg *.jpeg *.bmp)\")\n",
    "        if imagePath:\n",
    "            self.image_path = imagePath\n",
    "            pixmap = QtGui.QPixmap(imagePath)\n",
    "            pixmap = pixmap.scaled(self.selectedImageLbl.width(), self.selectedImageLbl.height(), QtCore.Qt.KeepAspectRatio)\n",
    "            self.selectedImageLbl.setPixmap(pixmap)\n",
    "            self.selectedImageLbl.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "    def recognizeEmotion(self):\n",
    "        if self.model_path and self.image_path:\n",
    "            from keras.preprocessing import image\n",
    "            from keras.models import load_model\n",
    "            import numpy as np\n",
    "            import datetime\n",
    "            emotions = ('злость', 'счастье', 'грусть', 'удивление')\n",
    "\n",
    "            img = image.load_img(self.image_path, target_size = (96, 96))\n",
    "            image_array = image.img_to_array(img)\n",
    "            image_array = np.expand_dims(image_array, axis = 0) / 255\n",
    "\n",
    "            model = load_model(self.model_path)\n",
    "            predictions = model.predict(image_array)\n",
    "            max_emotion_percent = '{:.2f}'.format(np.amax(predictions) * 100)\n",
    "            max_emotion_index = np.argmax(predictions)\n",
    "            recognized_emotion = emotions[max_emotion_index].capitalize() + ' (' + max_emotion_percent + '%)'\n",
    "            self.emotionNameLbl.setText(recognized_emotion)\n",
    "\n",
    "            with open(os.path.join(sys.path[0], \"emotions_log.txt\"), \"a+\") as f:\n",
    "                now = datetime.datetime.now()\n",
    "                f.write('Модель: ' + ntpath.basename(self.model_path) +\n",
    "                        ';   Изображение: ' + ntpath.basename(self.image_path) +\n",
    "                        ';   Эмоция: ' + recognized_emotion +\n",
    "                        ';   Дата: ' + now.strftime(\"%Y-%m-%d %H:%M:%S\") + '\\n')\n",
    "                f.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    MainWindow = QtWidgets.QMainWindow()\n",
    "    ui = Ui_MainWindow()\n",
    "    ui.setupUi(MainWindow)\n",
    "    MainWindow.show()\n",
    "    app.exec_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код основной логики приложения emotion_recognizer.py\n",
    "\n",
    "# import ntpath\n",
    "# import os\n",
    "\n",
    "\n",
    "#     model_path, image_path = \"\", \"\"\n",
    "\n",
    "\n",
    "# #         self.selectModelBtn.clicked.connect(self.setModel)\n",
    "#         self.selectImageBtn.clicked.connect(self.setImage)\n",
    "#         self.recognizeEmotionBtn.clicked.connect(self.recognizeEmotion)\n",
    "\n",
    "#         for file in os.listdir(sys.path[0]):\n",
    "#             file_path = os.path.join(sys.path[0], file)\n",
    "#             if os.path.isfile(file_path) & (file.endswith('.h5')):\n",
    "#                 self.model_path = file_path\n",
    "#                 self.modelNameLbl.setText(file)\n",
    "\n",
    "\n",
    "# #     def setModel(self):\n",
    "# #         modelPath, _ = QtWidgets.QFileDialog.getOpenFileName(None, \"Select Model\", \"\", \"FER Models (*.h5)\")\n",
    "# #         if modelPath:\n",
    "# #             self.model_path = modelPath\n",
    "# #             self.modelNameLbl.setText(ntpath.basename(modelPath))\n",
    "\n",
    "#     def setImage(self):\n",
    "#         imagePath, _ = QtWidgets.QFileDialog.getOpenFileName(None, \"Select Image\", \"\", \"Image Files (*.png *.jpg *.jpeg *.bmp)\")\n",
    "#         if imagePath:\n",
    "#             self.image_path = imagePath\n",
    "#             pixmap = QtGui.QPixmap(imagePath)\n",
    "#             pixmap = pixmap.scaled(self.selectedImageLbl.width(), self.selectedImageLbl.height(), QtCore.Qt.KeepAspectRatio)\n",
    "#             self.selectedImageLbl.setPixmap(pixmap)\n",
    "#             self.selectedImageLbl.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "#     def recognizeEmotion(self):\n",
    "#         if self.model_path and self.image_path:\n",
    "#             from keras.preprocessing import image\n",
    "#             from keras.models import load_model\n",
    "#             import numpy as np\n",
    "#             import datetime\n",
    "#             emotions = ('злость', 'счастье', 'грусть', 'удивление')\n",
    "\n",
    "#             img = image.load_img(self.image_path, target_size = (96, 96))\n",
    "#             image_array = image.img_to_array(img)\n",
    "#             image_array = np.expand_dims(image_array, axis = 0) / 255\n",
    "\n",
    "#             model = load_model(self.model_path)\n",
    "#             predictions = model.predict(image_array)\n",
    "#             max_emotion_percent = '{:.2f}'.format(np.amax(predictions) * 100)\n",
    "#             max_emotion_index = np.argmax(predictions)\n",
    "#             recognized_emotion = emotions[max_emotion_index].capitalize() + ' (' + max_emotion_percent + '%)'\n",
    "#             self.emotionNameLbl.setText(recognized_emotion)\n",
    "\n",
    "#             with open(os.path.join(sys.path[0], \"emotions_log.txt\"), \"a+\") as f:\n",
    "#                 now = datetime.datetime.now()\n",
    "#                 f.write('Модель: ' + ntpath.basename(self.model_path) +\n",
    "#                         ';   Изображение: ' + ntpath.basename(self.image_path) +\n",
    "#                         ';   Эмоция: ' + recognized_emotion +\n",
    "#                         ';   Дата: ' + now.strftime(\"%Y-%m-%d %H:%M:%S\") + '\\n')\n",
    "#                 f.close()\n",
    "\n",
    "\n",
    "#     app.exec_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полезный код\n",
    "\n",
    "# %load emotion_recognizer.py\n",
    "# %%writefile emotion_recognizer.py\n",
    "# pyuic5 -x C:\\Users\\TheDeGe\\Desktop\\emotion_recognizer.ui -o C:\\Users\\TheDeGe\\Desktop\\emotion_recognizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Обычная CNN\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "\n",
    "number_of_epochs = 100\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('ferCNN_simple.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_images,\n",
    "#     train_labels,\n",
    "#     epochs=number_of_epochs,\n",
    "#     batch_size=batch_size,\n",
    "#     validation_data=(validation_images, validation_labels))\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=math.ceil(num_train_samples / batch_size),\n",
    "    epochs=number_of_epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=math.ceil(num_val_samples / batch_size),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "# model.save('ferCNN_simple.h5')\n",
    "\n",
    "# val_loss, val_acc = model.evaluate(validation_images, validation_labels)\n",
    "# print('val_acc:', val_acc)\n",
    "# print('val_loss:', val_loss)\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=math.ceil(num_test_samples / batch_size))\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расшифровка меток эмоций: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
    "# Отображает информацию о датасете\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# dataset_dir = 'datasets/fer2013.csv'\n",
    "# dataset = pd.read_csv(dataset_dir)\n",
    "\n",
    "# print(dataset.head())\n",
    "# print('\\n')\n",
    "# print(dataset[\"Usage\"].value_counts())\n",
    "# print('\\n')\n",
    "# print(dataset[\"emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказывает эмоцию по изображению и выводит график предсказаний\n",
    "\n",
    "# %matplotlib inline\n",
    "# from keras.preprocessing import image\n",
    "# from keras.models import load_model\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# model_path = 'ferCNN_vgg16_fr.h5'\n",
    "# image_path = '123.jpg'\n",
    "\n",
    "# def emotion_analysis(predictions):\n",
    "#     emotions = ('angry', 'happy', 'sad', 'surprise')\n",
    "#     max_em_index = np.argmax(predictions)\n",
    "#     y_pos = np.arange(len(emotions))\n",
    "    \n",
    "#     plt.bar(y_pos, predictions[0], align='center', alpha=0.5)\n",
    "#     plt.xticks(y_pos, emotions)\n",
    "#     plt.ylabel('Percentage')\n",
    "#     plt.title('Emotion: ' + emotions[max_em_index].capitalize())\n",
    "# #     plt.savefig('emotion_analysis.png', bbox_inches='tight')\n",
    "#     plt.show()\n",
    "\n",
    "# full_image = image.load_img(image_path)\n",
    "# img = full_image.resize((96, 96))\n",
    "# image_array = image.img_to_array(img)\n",
    "# image_array = np.expand_dims(image_array, axis = 0) / 255\n",
    "\n",
    "# model = load_model(model_path)\n",
    "# predictions = model.predict(image_array)\n",
    "# emotion_analysis(predictions)\n",
    "\n",
    "# plt.imshow(full_image)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отображает 4 варианта расширения данных первого изображения из датасета\n",
    "\n",
    "# %matplotlib inline\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.preprocessing import image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# imagegen = ImageDataGenerator(\n",
    "#     rotation_range=40,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode='nearest')\n",
    "\n",
    "# images_for_imagegen = train_images[0].reshape((1,) + train_images[0].shape)\n",
    "# i = 0\n",
    "\n",
    "# for batch in imagegen.flow(images_for_imagegen, batch_size=1):\n",
    "#     plt.figure(i)\n",
    "#     imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "#     plt.axis(\"off\")\n",
    "#     i += 1\n",
    "#     if i % 4 == 0:\n",
    "#         break\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отображает изображения из датасета\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# dataset_dir = 'datasets/fer2013.csv'\n",
    "# dataset_plus_dir = 'datasets/fer2013new.csv'\n",
    "# dataset = pd.read_csv(dataset_dir)\n",
    "# dataset_plus = pd.read_csv(dataset_plus_dir)\n",
    "# emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "\n",
    "# def show_image_and_label(x, y, i):\n",
    "#     x_reshaped = x.reshape(48,48)\n",
    "#     plt.imshow(x_reshaped, cmap= \"gray\", interpolation=\"nearest\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "#     print(i, emotions[y])\n",
    "\n",
    "# # drop_indices = dataset_plus.index[\n",
    "# # #     (dataset_plus['neutral'] < 10) &\n",
    "# #       (dataset_plus['happiness'] < 10)\n",
    "# #     & (dataset_plus['surprise'] < 9)\n",
    "# #     & (dataset_plus['sadness'] < 7)\n",
    "# #     & (dataset_plus['anger'] < 7)\n",
    "# # #     & (dataset_plus['disgust'] < 10)\n",
    "# # #     & (dataset_plus['fear'] < 10)\n",
    "# #     | (dataset_plus['contempt'] > 1)\n",
    "# #     | (dataset_plus['unknown'] > 1)\n",
    "# #     | (dataset_plus['NF'] > 0)]\n",
    "# # dataset = dataset.drop(drop_indices, axis=0)\n",
    "# # drop_indices = dataset.index[(dataset['emotion'] == 1) | (dataset['emotion'] == 2) | (dataset['emotion'] == 6)]\n",
    "# # dataset = dataset.drop(drop_indices, axis=0)\n",
    "# # drop_indices = np.random.choice(dataset.index[dataset['emotion'] == 3], 3800, replace=False)\n",
    "# # dataset = dataset.drop(drop_indices, axis=0)\n",
    "\n",
    "# # number_of_images = 0\n",
    "# # i = 0\n",
    "# # for img in dataset[\"pixels\"][:number_of_images]:\n",
    "# #     val = img.split(\" \")\n",
    "# #     x_pixels = np.array(val, 'float32') / 255\n",
    "# #     y_emotion = dataset[\"emotion\"][i]\n",
    "# #     show_image_and_label(x_pixels, y_emotion)\n",
    "# #     i += 1\n",
    "\n",
    "# # image_index = 0\n",
    "# # img = dataset[\"pixels\"][image_index]\n",
    "# # val = img.split(\" \")\n",
    "# # x_pixels = np.array(val, 'float32') / 255\n",
    "# # y_emotion = dataset[\"emotion\"][image_index]\n",
    "# # show_image_and_label(x_pixels, y_emotion)\n",
    "\n",
    "# i = 0\n",
    "# for index, row in dataset_plus.iterrows():\n",
    "#     if row['NF'] == 10:\n",
    "#         img = dataset[\"pixels\"][index]\n",
    "#         val = img.split(\" \")\n",
    "#         x_pixels = np.array(val, 'float32') / 255\n",
    "#         y_emotion = dataset[\"emotion\"][index]\n",
    "#         show_image_and_label(x_pixels, y_emotion, index)\n",
    "#         i += 1\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводит данные из датасета в картинки и сохраняет их по трём папкам:\n",
    "# 'Training', 'PublicTest', 'PrivateTest' внутри четвёртой: 'fer2013_images' рядом c этим файлом\n",
    "\n",
    "# import os\n",
    "# import csv\n",
    "# import numpy as np\n",
    "# import imageio\n",
    "\n",
    "# dataset_dir = 'datasets/fer2013.csv'\n",
    "# h, w = 48, 48\n",
    "# id = 0\n",
    "\n",
    "# with open(dataset_dir, 'r') as csvfile:\n",
    "#     data_reader = csv.reader(csvfile, delimiter =',')\n",
    "#     headers = next(data_reader)\n",
    "    \n",
    "#     for row in data_reader:\n",
    "#         pixels = row[1].split()\n",
    "#         usage = row[2]\n",
    "        \n",
    "#         image = np.asarray(pixels, dtype= np.uint8).reshape(h, w)\n",
    "#         stacked_image = np.dstack((image,) * 3)\n",
    "        \n",
    "#         image_folder = os.path.join('fer2013_images', usage)\n",
    "#         if not os.path.exists(image_folder):\n",
    "#             os.makedirs(image_folder)\n",
    "            \n",
    "#         image_file_name = os.path.join(image_folder, str(id) + '.jpg')\n",
    "#         imageio.imwrite(image_file_name, stacked_image)\n",
    "        \n",
    "#         id += 1\n",
    "#         if id % 1000 == 0:\n",
    "#             print('Processed {} images'.format(id))\n",
    "    \n",
    "#     print(\"Finished processing {} images\".format(id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
